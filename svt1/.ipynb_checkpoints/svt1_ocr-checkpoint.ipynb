{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "prescribed-likelihood",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from datetime import datetime\n",
    "\n",
    "def get_timestamp() -> str:\n",
    "    timestamp = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "    return timestamp\n",
    "\n",
    "get_timestamp()\n",
    "\n",
    "\n",
    "from tensorflow_addons.optimizers import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "absent-machinery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = Path(\"./crop/\")\n",
    "\n",
    "# # Get list of all the images\n",
    "# images = sorted(list(map(str, list(data_dir.glob(\"*.jpg\")))))\n",
    "# # labels = [img.split(os.path.sep)[-1].split(\".jpg\")[0] for img in images]\n",
    "# for img in images:\n",
    "# #     print(img.split(os.path.sep)[-1].split(\".jpg\")[0])\n",
    "#     print(img.split(os.path.sep)[-1].split(\".jpg\")[0][5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "square-sculpture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images found:  239\n",
      "Number of labels found:  239\n",
      "Number of unique characters:  26\n",
      "Characters present:  {'T', 'O', 'Z', 'M', 'E', 'I', 'N', 'X', 'D', 'Q', 'W', 'S', 'R', 'C', 'J', 'A', 'K', 'F', 'Y', 'V', 'L', 'U', 'B', 'H', 'P', 'G'}\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(\"./crop/\")\n",
    "\n",
    "# Get list of all the images\n",
    "images = sorted(list(map(str, list(data_dir.glob(\"*.jpg\")))))\n",
    "labels = [img.split(os.path.sep)[-1].split(\".jpg\")[0][5:] for img in images]\n",
    "characters = set(char for label in labels for char in label)\n",
    "characters = set(sorted(list(characters)))\n",
    "\n",
    "print(\"Number of images found: \", len(images))\n",
    "print(\"Number of labels found: \", len(labels))\n",
    "print(\"Number of unique characters: \", len(characters))\n",
    "print(\"Characters present: \", characters)\n",
    "\n",
    "# Batch size for training and validation\n",
    "batch_size = 16\n",
    "# Desired image dimensions\n",
    "img_width = 512\n",
    "img_height = 64\n",
    "# Factor by which the image is going to be downsampled\n",
    "# by the convolutional blocks. We will be using two\n",
    "# convolution blocks and each block will have\n",
    "# a pooling layer which downsample the features by a factor of 2.\n",
    "# Hence total downsampling factor would be 4.\n",
    "downsample_factor = 4\n",
    "\n",
    "# Maximum length of any captcha in the dataset\n",
    "max_length = max([len(label) for label in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "executive-yield",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'T': 0, 'O': 1, 'Z': 2, 'M': 3, 'E': 4, 'I': 5, 'N': 6, 'X': 7, 'D': 8, 'Q': 9, 'W': 10, 'S': 11, 'R': 12, 'C': 13, 'J': 14, 'A': 15, 'K': 16, 'F': 17, 'Y': 18, 'V': 19, 'L': 20, 'U': 21, 'B': 22, 'H': 23, 'P': 24, 'G': 25}\n"
     ]
    }
   ],
   "source": [
    "char2id = {}\n",
    "id2char = {}\n",
    "for i,x in enumerate(list(characters)):\n",
    "    char2id[x] = i\n",
    "    \n",
    "print(char2id)\n",
    "for i,x in enumerate(list(characters)):\n",
    "    id2char[i] = x\n",
    "    \n",
    "# print(id2char)\n",
    "\n",
    "def char_to_num(char):\n",
    "#     res=[]\n",
    "    res = np.ones(15) - 2\n",
    "    for i,c in enumerate(char):\n",
    "#         print(c)\n",
    "#         res.append(char2id[c])\n",
    "        res[i] = char2id[c]\n",
    "    return res\n",
    "\n",
    "def num_to_char(num):\n",
    "    res = []\n",
    "    for n in num.numpy():\n",
    "        if n in id2char.keys():\n",
    "            res.append(id2char[n])\n",
    "        else:\n",
    "            res.append('?')\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "accepted-induction",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_data(images, labels, train_size=0.9, shuffle=True):\n",
    "    # 1. Get the total size of the dataset\n",
    "    size = len(images) \n",
    "    # 2. Make an indices array and shuffle it, if required\n",
    "    indices = np.arange(size)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    # 3. Get the size of training samples\n",
    "    train_samples = int(size * train_size)\n",
    "    # 4. Split data into training and validation sets\n",
    "    x_train, y_train = images[indices[:train_samples]], labels[indices[:train_samples]]\n",
    "    x_valid, y_valid = images[indices[train_samples:]], labels[indices[train_samples:]]\n",
    "    return x_train, x_valid, y_train, y_valid\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Splitting data into training and validation sets\n",
    "x_train, x_valid, c_train, c_valid = split_data(np.array(images), np.array(labels))\n",
    "\n",
    "y_train=[]\n",
    "for i in c_train:\n",
    "    y_train.append(char_to_num(i))\n",
    "# y_train = np.array(y_train,dtype=np.int32)\n",
    "# y_valid=[]\n",
    "# for i in c_valid:\n",
    "#     y_valid.append(char_to_num(i))\n",
    "# y_valid = np.array(y_valid,dtype=np.int32)\n",
    "# y_train\n",
    "\n",
    "\n",
    "y_valid=[]\n",
    "for i in c_valid:\n",
    "#     break\n",
    "    y_valid.append(char_to_num(i))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statistical-weight",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustained-oxygen",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "academic-baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def encode_single_sample(img_path, label):\n",
    "    # 1. Read image\n",
    "    img = tf.io.read_file(img_path)\n",
    "    # 2. Decode and convert to grayscale\n",
    "    img = tf.io.decode_png(img, channels=3)\n",
    "    # 3. Convert to float32 in [0, 1] range\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # 4. Resize to the desired size\n",
    "#     img = tf.image.resize(img, [img_height, h/3])\n",
    "    # 5. Transpose the image because we want the time\n",
    "    # dimension to correspond to the width of the image.\n",
    "#     img = tf.transpose(img, perm=[1, 0, 2])\n",
    "    # 6. Map the characters in label to numbers\n",
    "#     label = char_to_label(label)\n",
    "#     label = char_to_num(tf.strings.unicode_split(label, input_encoding=\"UTF-8\")) -1\n",
    "    # 7. Return a dict as our model is expecting two inputs\n",
    "    lb_len = tf.reduce_sum(tf.cast(tf.not_equal(label, -1), tf.int64))\n",
    "#     input_len = tf.math\n",
    "#     return {\"image\": img, \"label\": label, \"label_len\":lb_len, \"input_len\":input_len}\n",
    "    return {\"image\": img, \"label\": label, \"label_len\": lb_len}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-tunisia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "worth-thesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_path = x_train[0]\n",
    "# label = y_train[0]\n",
    "# print(img_path)\n",
    "# img = tf.io.read_file(img_path)\n",
    "# # 2. Decode and convert to grayscale\n",
    "# img = tf.io.decode_png(img, channels=3)\n",
    "# # 3. Convert to float32 in [0, 1] range\n",
    "# img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "# w,h = img.shape[0],img.shape[1]\n",
    "# # 4. Resize to the desired size\n",
    "# #     img = tf.image.resize(img, [img_height, img_width])\n",
    "# # 5. Transpose the image because we want the time\n",
    "# # dimension to correspond to the width of the image.\n",
    "# #     img = tf.transpose(img, perm=[1, 0, 2])\n",
    "# # 6. Map the characters in label to numbers\n",
    "# #     label = char_to_label(label)\n",
    "# #     label = char_to_num(tf.strings.unicode_split(label, input_encoding=\"UTF-8\")) -1\n",
    "# # 7. Return a dict as our model is expecting two inputs\n",
    "# lb_len = tf.reduce_sum(tf.cast(tf.not_equal(label, -1), tf.int64))\n",
    "# input_len = w // int(h // 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "elect-compilation",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((x_valid, y_valid))\n",
    "validation_dataset = validation_dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "following-incident",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "approved-explanation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24. 15. 18. 20.  4. 11. 11. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "tf.Tensor(7, shape=(), dtype=int64)\n",
      "PAYLESS????????\n",
      "[ 8. 15. 18. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "tf.Tensor(3, shape=(), dtype=int64)\n",
      "DAY????????????\n",
      "[20.  5. 19.  5.  6. 25. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n",
      "LIVING?????????\n",
      "[22. 20. 21.  4. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "BLUE???????????\n",
      "[ 0. 23.  4. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "tf.Tensor(3, shape=(), dtype=int64)\n",
      "THE????????????\n",
      "[20.  1. 20. 15. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "LOLA???????????\n",
      "[24. 15.  6. 13. 15. 16.  4. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "tf.Tensor(7, shape=(), dtype=int64)\n",
      "PANCAKE????????\n",
      "[10.  4. 11.  0.  4. 12.  6. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "tf.Tensor(7, shape=(), dtype=int64)\n",
      "WESTERN????????\n",
      "[ 9. 21.  5.  6.  6. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "tf.Tensor(5, shape=(), dtype=int64)\n",
      "QUINN??????????\n",
      "[10. 15. 20.  6. 21.  0. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n",
      "WALNUT?????????\n",
      "[11. 24. 15. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "tf.Tensor(3, shape=(), dtype=int64)\n",
      "SPA????????????\n",
      "[22.  4. 11.  0. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "BEST???????????\n",
      "[18. 15. 12.  8. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "YARD???????????\n",
      "[ 8.  1.  3.  5.  6.  5. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n",
      "DOMINI?????????\n",
      "[11.  5.  6. 25. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "SING???????????\n",
      "[ 3. 15. 12. 16.  4.  0. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "tf.Tensor(6, shape=(), dtype=int64)\n",
      "MARKET?????????\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, ax = plt.subplots(4, 4, figsize=(10, 5))\n",
    "for batch in train_dataset.take(1):\n",
    "    images = batch[\"image\"]\n",
    "    labels = batch[\"label\"]\n",
    "    label_len = batch[\"label_len\"]\n",
    "    for i in range(16):\n",
    "        img = (images[i]).numpy().astype(\"float32\")\n",
    "#         print(labels[i])\n",
    "        print(labels[i].numpy())\n",
    "        print(label_len[i])\n",
    "        label = ''.join(num_to_char(labels[i]))\n",
    "        print(label)\n",
    "        ax[i // 4, i % 4].imshow(img[:, :, 0], cmap=\"gray\")\n",
    "        ax[i // 4, i % 4].set_title(label)\n",
    "        ax[i // 4, i % 4].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broken-resident",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_len[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "hybrid-discharge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 16, 128, 64), dtype=tf.float32, name=None), name='max_pooling2d/MaxPool:0', description=\"created by layer 'max_pooling2d'\")\n",
      "Tensor(\"Placeholder_2:0\", shape=(None,), dtype=int64)\n",
      "Model: \"ocr_model_v1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "image (InputLayer)              [(None, 64, 512, 3)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Conv2 (Conv2D)                  (None, 64, 512, 64)  1792        image[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "pool2 (MaxPooling2D)            (None, 32, 256, 64)  0           Conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 32, 256, 64)  36928       pool2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 16, 128, 64)  0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 1, 128, 32)   32800       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "logits (Lambda)                 (None, 128, 32)      0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "label (InputLayer)              [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 128, 27)      891         logits[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "label_len (InputLayer)          [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ctc_loss (CTCLayer)             (None, 128, 27)      0           label[0][0]                      \n",
      "                                                                 dense2[0][0]                     \n",
      "                                                                 label_len[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 72,411\n",
      "Trainable params: 72,411\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "#     \"alpha\": 1.0,\n",
    "#     \"minimalistic\": True,\n",
    "    \"weights\": \"imagenet\",\n",
    "    \"weights\": None,\n",
    "    \"include_top\": False\n",
    "}\n",
    "class CTCLayer(layers.Layer):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.loss_fn = keras.backend.ctc_batch_cost\n",
    "\n",
    "    def call(self, y_true, y_pred,label_length):\n",
    "        # Compute the training-time loss value and add it\n",
    "        # to the layer using `self.add_loss()`.\n",
    "        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
    "        input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
    "#         label_length = tf.cast(label_length, dtype=\"int64\")\n",
    "#         label_length = tf.cast(label_length, tf.int32)\n",
    "        print(label_length)\n",
    "        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "#         label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "        loss = self.loss_fn(y_true, y_pred, input_length, label_length[:,tf.newaxis])\n",
    "        self.add_loss(loss)\n",
    "        # At test time, just return the computed predictions\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    # Inputs to the model\n",
    "    input_img = layers.Input(\n",
    "        shape=(img_height,img_width, 3), name=\"image\", dtype=\"float32\"\n",
    "#         shape=(200, 64, 3), name=\"image\", dtype=\"float32\"\n",
    "    )\n",
    "    # rescale = layers.Rescaling(scale=1.0 / 255,offset=0)(input_img)\n",
    "    labels = layers.Input(name=\"label\", shape=(None,), dtype=\"float32\")\n",
    "    label_len = keras.Input(shape=(), dtype=tf.int64, name='label_len')\n",
    "    # x = keras.applications.vgg19.VGG19(include_top=False,weights='imagenet', input_tensor=input_img).get_layer('block2_pool').output\n",
    "    # # base = keras.applications.MobileNetV3Large(input_tensor=input_img, **config).get_layer('Conv').output\n",
    "    # x = keras.applications.ResNet101(include_top=False, weights='imagenet', input_tensor=input_img).get_layer('conv2_block1_out').output\n",
    "#     x = keras.applications.MobileNetV3Large(input_tensor=input_img, **config).get_layer('expanded_conv/Add').output\n",
    "#     x  = keras.applications.resnet50.ResNet50(input_tensor=input_img, **config).get_layer('conv1_conv').output\n",
    "#     x = keras.layers.BatchNormalization()(x)\n",
    "#     x = keras.layers.ReLU()(x)\n",
    "#     x = keras.layers.DepthwiseConv2D(32,(3,3),activation=\"relu\",padding=\"same\",kernel_initializer=\"he_normal\")(x)\n",
    "\n",
    "    # base = keras.applications.MobileNetV3Large(input_tensor=input_img,**config)\n",
    "    # base = keras.applications.MobileNet(input_tensor=input_img, **config)\n",
    "\n",
    "    # x = base.get_layer('Conv')(input_img)\n",
    "    # x = base.get_layer('conv1').output\n",
    "    \n",
    "\n",
    "    # feature = base.get_layer('Conv').output\n",
    "\n",
    "    # # First conv block\n",
    "#     x = layers.Conv2D(\n",
    "#         32,\n",
    "#         (3, 3),\n",
    "#         activation=\"relu\",\n",
    "#         kernel_initializer=\"he_normal\",\n",
    "#         padding=\"same\",\n",
    "#         name=\"Conv1\",\n",
    "#     )(x)\n",
    "#     # x = layers.MaxPooling2D((2, 2), name=\"pool1\")(x)\n",
    "\n",
    "#     # # Second conv block\n",
    "    x = layers.Conv2D(\n",
    "        64,\n",
    "        (3, 3),\n",
    "        activation=\"relu\",\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        padding=\"same\",\n",
    "        name=\"Conv2\",\n",
    "    )(input_img)\n",
    "    x = layers.MaxPooling2D((2, 2), name=\"pool2\")(x)\n",
    "#     x = layers.Dropout(0.2)(x)\n",
    "\n",
    "    x = layers.Conv2D(\n",
    "        64,\n",
    "        (3, 3),\n",
    "        activation=\"relu\",\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        padding=\"same\",\n",
    "    )(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    print(x)\n",
    "    x = layers.Conv2D(32,(16,1),activation=\"relu\",strides=1)(x)\n",
    "    \n",
    "    x = layers.Lambda(lambda x: tf.squeeze(x, 1), name='logits')(x)\n",
    "    # We have used two max pool with pool size and strides 2.\n",
    "    # Hence, downsampled feature maps are 4x smaller. The number of\n",
    "    # filters in the last layer is 64. Reshape accordingly before\n",
    "    # passing the output to the RNN part of the model\n",
    "    # reshape\n",
    "    # new_shape = ((img_width // 4), (img_height // 4) * 64)\n",
    "    # x = layers.Reshape(target_shape=new_shape, name=\"reshape\")(x)\n",
    "\n",
    "    # x = layers.Dense(64, activation=\"relu\", name=\"dense1\")(x)\n",
    "    # x = layers.Dropout(0.2)(x)\n",
    "\n",
    "    # RNNs\n",
    "    # x = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.25))(x)\n",
    "    # x = layers.Bidirectional(layers.LSTM(64, return_sequences=True, dropout=0.25))(x)\n",
    "\n",
    "    # Output layer\n",
    "    x = layers.Dense(\n",
    "        27, activation=\"softmax\", name=\"dense2\"\n",
    "    )(x)\n",
    "    # Add CTC layer for calculating CTC loss at each step\n",
    "    output = CTCLayer(name=\"ctc_loss\")(labels, x,label_len)\n",
    "    # Define the model\n",
    "    model = keras.models.Model(\n",
    "        inputs=[input_img, labels,label_len], outputs=output, name=\"ocr_model_v1\"\n",
    "    )\n",
    "    # Optimizer\n",
    "    opt = keras.optimizers.Adam(lr=0.0012, beta_1=0.9, beta_2=0.9, epsilon=1e-08, amsgrad=True) \n",
    "    # Compile the model and return\n",
    "    model.compile(optimizer=opt)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Get the model\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "regulation-complex",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 16, 128, 64), dtype=tf.float32, name=None), name='pool2/MaxPool:0', description=\"created by layer 'pool2'\")\n",
      "Model: \"ocr_model_v1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "image (InputLayer)              [(None, 64, 512, 3)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Conv1 (Conv2D)                  (None, 64, 512, 32)  896         image[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "pool1 (MaxPooling2D)            (None, 32, 256, 32)  0           Conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Conv2 (Conv2D)                  (None, 32, 256, 64)  18496       pool1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "pool2 (MaxPooling2D)            (None, 16, 128, 64)  0           Conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 1, 128, 32)   32800       pool2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "logits (Lambda)                 (None, 128, 32)      0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 128, 64)      2112        logits[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 128, 64)      0           dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "label (InputLayer)              [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 128, 27)      1755        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "label_len (InputLayer)          [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ctc_loss (CTCLayer)             (None, 128, 27)      0           label[0][0]                      \n",
      "                                                                 dense2[0][0]                     \n",
      "                                                                 label_len[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 56,059\n",
      "Trainable params: 56,059\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class CTCLayer(layers.Layer):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.loss_fn = keras.backend.ctc_batch_cost\n",
    "\n",
    "    def call(self, y_true, y_pred,label_length):\n",
    "        # Compute the training-time loss value and add it\n",
    "        # to the layer using `self.add_loss()`.\n",
    "        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
    "        input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
    "#         label_length = tf.cast(label_length, dtype=\"int64\")\n",
    "#         label_length = tf.cast(label_length, tf.int32)\n",
    "#         print(label_length)\n",
    "        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "#         label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "        loss = self.loss_fn(y_true, y_pred, input_length, label_length[:,tf.newaxis])\n",
    "        self.add_loss(loss)\n",
    "        # At test time, just return the computed predictions\n",
    "        return y_pred\n",
    "\n",
    "def build_model():\n",
    "    # Inputs to the model\n",
    "    input_img = layers.Input(\n",
    "        shape=(img_height,img_width, 3), name=\"image\", dtype=\"float32\"\n",
    "#         shape=(200, 64, 3), name=\"image\", dtype=\"float32\"\n",
    "    )\n",
    "    labels = layers.Input(name=\"label\", shape=(None,), dtype=\"float32\")\n",
    "    label_len = keras.Input(shape=(), dtype=tf.int64, name='label_len')\n",
    "\n",
    "    # First conv block\n",
    "    x = layers.Conv2D(\n",
    "        32,\n",
    "        (3, 3),\n",
    "        activation=\"relu\",\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        padding=\"same\",\n",
    "        name=\"Conv1\",\n",
    "    )(input_img)\n",
    "    x = layers.MaxPooling2D((2, 2), name=\"pool1\")(x)\n",
    "\n",
    "    # Second conv block\n",
    "    x = layers.Conv2D(\n",
    "        64,\n",
    "        (3, 3),\n",
    "        activation=\"relu\",\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        padding=\"same\",\n",
    "        name=\"Conv2\",\n",
    "    )(x)\n",
    "    x = layers.MaxPooling2D((2, 2), name=\"pool2\")(x)\n",
    "\n",
    "    # We have used two max pool with pool size and strides 2.\n",
    "    # Hence, downsampled feature maps are 4x smaller. The number of\n",
    "    # filters in the last layer is 64. Reshape accordingly before\n",
    "    # passing the output to the RNN part of the model\n",
    "#     new_shape = ((img_width // 4), (img_height // 4) * 64)\n",
    "#     x = layers.Reshape(target_shape=new_shape, name=\"reshape\")(x)\n",
    "    print(x)\n",
    "#     x = layers.Conv2D(32,(16,1),activation=\"relu\",strides=1)(x)\n",
    "#     x = layers.Lambda(lambda x: tf.squeeze(x, 1), name='logits')(x)\n",
    "    \n",
    "    x = layers.Conv2D(32,(16,1),activation=\"relu\",strides=1)(x)\n",
    "    x = layers.Lambda(lambda x: tf.squeeze(x, 1), name='logits')(x)\n",
    "    \n",
    "    \n",
    "    x = layers.Dense(64, activation=\"relu\", name=\"dense1\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "\n",
    "    # RNNs\n",
    "#     x = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.25))(x)\n",
    "#     x = layers.Bidirectional(layers.LSTM(64, return_sequences=True, dropout=0.25))(x)\n",
    "\n",
    "    # Output layer\n",
    "    x = layers.Dense(\n",
    "        27, activation=\"softmax\", name=\"dense2\"\n",
    "    )(x)\n",
    "\n",
    "    # Add CTC layer for calculating CTC loss at each step\n",
    "    output = CTCLayer(name=\"ctc_loss\")(labels, x,label_len)\n",
    "\n",
    "    # Define the model\n",
    "    model = keras.models.Model(\n",
    "        inputs=[input_img, labels,label_len], outputs=output, name=\"ocr_model_v1\"\n",
    "    )\n",
    "    \n",
    "#     model = keras.models.Model(\n",
    "#         inputs=[input_img, labels], outputs=output, name=\"ocr_model_v1\"\n",
    "#     )\n",
    "    # Optimizer\n",
    "    opt = keras.optimizers.Adam()\n",
    "    # Compile the model and return\n",
    "    model.compile(optimizer=opt)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Get the model\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "first-partner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/100\n",
      "14/14 [==============================] - 3s 165ms/step - loss: 348.6972 - val_loss: 206.9887\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 206.98871, saving model to 2021_10_28_06_49_21.ckpt\n",
      "Epoch 2/100\n",
      "14/14 [==============================] - 2s 143ms/step - loss: 161.3865 - val_loss: 25.4305\n",
      "\n",
      "Epoch 00002: val_loss improved from 206.98871 to 25.43053, saving model to 2021_10_28_06_49_21.ckpt\n",
      "Epoch 3/100\n",
      "14/14 [==============================] - 2s 154ms/step - loss: 33.4059 - val_loss: 40.2553\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 25.43053\n",
      "Epoch 4/100\n",
      "14/14 [==============================] - 2s 149ms/step - loss: 31.4651 - val_loss: 21.5867\n",
      "\n",
      "Epoch 00004: val_loss improved from 25.43053 to 21.58665, saving model to 2021_10_28_06_49_21.ckpt\n",
      "Epoch 5/100\n",
      "14/14 [==============================] - 2s 146ms/step - loss: 24.6568 - val_loss: 20.5684\n",
      "\n",
      "Epoch 00005: val_loss improved from 21.58665 to 20.56841, saving model to 2021_10_28_06_49_21.ckpt\n",
      "Epoch 6/100\n",
      "14/14 [==============================] - 2s 141ms/step - loss: 22.7839 - val_loss: 19.9688\n",
      "\n",
      "Epoch 00006: val_loss improved from 20.56841 to 19.96884, saving model to 2021_10_28_06_49_21.ckpt\n",
      "Epoch 7/100\n",
      "14/14 [==============================] - 2s 141ms/step - loss: 21.9671 - val_loss: 19.0694\n",
      "\n",
      "Epoch 00007: val_loss improved from 19.96884 to 19.06935, saving model to 2021_10_28_06_49_21.ckpt\n",
      "Epoch 8/100\n",
      "14/14 [==============================] - 2s 139ms/step - loss: 21.8062 - val_loss: 19.1617\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 19.06935\n",
      "Epoch 9/100\n",
      "14/14 [==============================] - 2s 141ms/step - loss: 21.7797 - val_loss: 18.8708\n",
      "\n",
      "Epoch 00009: val_loss improved from 19.06935 to 18.87080, saving model to 2021_10_28_06_49_21.ckpt\n",
      "Epoch 10/100\n",
      "14/14 [==============================] - 2s 140ms/step - loss: 21.5675 - val_loss: 18.8668\n",
      "\n",
      "Epoch 00010: val_loss improved from 18.87080 to 18.86680, saving model to 2021_10_28_06_49_21.ckpt\n",
      "Epoch 11/100\n",
      "14/14 [==============================] - 2s 138ms/step - loss: 21.5255 - val_loss: 18.8176\n",
      "\n",
      "Epoch 00011: val_loss improved from 18.86680 to 18.81757, saving model to 2021_10_28_06_49_21.ckpt\n",
      "Epoch 12/100\n",
      "14/14 [==============================] - 2s 141ms/step - loss: 21.4859 - val_loss: 18.8317\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 18.81757\n",
      "Epoch 13/100\n",
      "14/14 [==============================] - 2s 135ms/step - loss: 21.4765 - val_loss: 18.7438\n",
      "\n",
      "Epoch 00013: val_loss improved from 18.81757 to 18.74382, saving model to 2021_10_28_06_49_21.ckpt\n",
      "Epoch 14/100\n",
      "14/14 [==============================] - 2s 134ms/step - loss: 21.4541 - val_loss: 18.7912\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 18.74382\n",
      "Epoch 15/100\n",
      "14/14 [==============================] - 2s 134ms/step - loss: 21.4693 - val_loss: 18.7664\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 18.74382\n",
      "Epoch 16/100\n",
      "14/14 [==============================] - 2s 139ms/step - loss: 21.5887 - val_loss: 18.7411\n",
      "\n",
      "Epoch 00016: val_loss improved from 18.74382 to 18.74113, saving model to 2021_10_28_06_49_21.ckpt\n",
      "Epoch 17/100\n",
      "14/14 [==============================] - 2s 135ms/step - loss: 21.4499 - val_loss: 18.6973\n",
      "\n",
      "Epoch 00017: val_loss improved from 18.74113 to 18.69727, saving model to 2021_10_28_06_49_21.ckpt\n",
      "Epoch 18/100\n",
      "14/14 [==============================] - 2s 133ms/step - loss: 21.4658 - val_loss: 18.7527\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 18.69727\n",
      "Epoch 19/100\n",
      "14/14 [==============================] - 2s 139ms/step - loss: 21.5121 - val_loss: 18.6601\n",
      "\n",
      "Epoch 00019: val_loss improved from 18.69727 to 18.66009, saving model to 2021_10_28_06_49_21.ckpt\n",
      "Epoch 20/100\n",
      "14/14 [==============================] - 2s 139ms/step - loss: 21.5003 - val_loss: 18.6756\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 18.66009\n",
      "Epoch 21/100\n",
      "14/14 [==============================] - 2s 135ms/step - loss: 21.4267 - val_loss: 18.6349\n",
      "\n",
      "Epoch 00021: val_loss improved from 18.66009 to 18.63493, saving model to 2021_10_28_06_49_21.ckpt\n",
      "Epoch 22/100\n",
      "14/14 [==============================] - 2s 133ms/step - loss: 21.3533 - val_loss: 18.7447\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 18.63493\n",
      "Epoch 23/100\n",
      "14/14 [==============================] - 2s 137ms/step - loss: 21.3693 - val_loss: 18.6564\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 18.63493\n",
      "Epoch 24/100\n",
      "14/14 [==============================] - 2s 140ms/step - loss: 21.5792 - val_loss: 18.6486\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 18.63493\n",
      "Epoch 25/100\n",
      "14/14 [==============================] - 2s 137ms/step - loss: 21.5625 - val_loss: 18.6564\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 18.63493\n",
      "Epoch 26/100\n",
      "14/14 [==============================] - 2s 137ms/step - loss: 21.4207 - val_loss: 18.6409\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 18.63493\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-e08cf25f0d3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1086\u001b[0m           self._maybe_load_initial_epoch_from_ckpt(initial_epoch))\n\u001b[1;32m   1087\u001b[0m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1088\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1089\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1136\u001b[0m           \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_recreate_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1138\u001b[0;31m           \u001b[0mdata_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1139\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    680\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcomponents\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0melement_spec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    684\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0;31m# Store dataset reference to ensure that dataset is alive when this iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_apply_options\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[0mautotune\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcpu_budget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mram_budget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_autotune_settings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mautotune\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m       \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ModelDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcpu_budget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mram_budget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;31m# (3) Apply graph rewrite options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, algorithm, cpu_budget, ram_budget)\u001b[0m\n\u001b[1;32m   4543\u001b[0m         \u001b[0mram_budget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mram_budget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4544\u001b[0m         **self._flat_structure)\n\u001b[0;32m-> 4545\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ModelDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, variant_tensor)\u001b[0m\n\u001b[1;32m   3121\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3122\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3123\u001b[0;31m     super(UnaryUnchangedStructureDataset, self).__init__(\n\u001b[0m\u001b[1;32m   3124\u001b[0m         input_dataset, variant_tensor)\n\u001b[1;32m   3125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, variant_tensor)\u001b[0m\n\u001b[1;32m   3110\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3111\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3112\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUnaryDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3114\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, variant_tensor)\u001b[0m\n\u001b[1;32m    216\u001b[0m       \u001b[0minput_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0minput_options\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options_attr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options_attr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(self, options)\u001b[0m\n\u001b[1;32m   3095\u001b[0m       \u001b[0mthe\u001b[0m \u001b[0minput\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptions\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3096\u001b[0m     \"\"\"\n\u001b[0;32m-> 3097\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0moptions_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3099\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/util/options.py\u001b[0m in \u001b[0;36mmerge_options\u001b[0;34m(*options_list)\u001b[0m\n\u001b[1;32m    142\u001b[0m       \u001b[0mthat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m       \u001b[0mdefault\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0mthis\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/util/options.py\u001b[0m in \u001b[0;36m__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     43\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/util/options.py\u001b[0m in \u001b[0;36mget_fn\u001b[0;34m(option)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moption\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m       \u001b[0moption\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moption\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "filepath = get_timestamp() + '.ckpt'\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
    "\n",
    "epochs = 100\n",
    "early_stopping_patience = 20\n",
    "# Add early stopping\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=early_stopping_patience, restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=[early_stopping,checkpoint],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metallic-hartford",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
